{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import logging as log\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "#import quickstart as gsh\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import os \n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "# The ID and range of the spreadsheet.\n",
    "SAMPLE_SPREADSHEET_ID = '1TpT5aOE1cQkqA2GOKRUN2vwXXHt_GD5JRPjGCVLvvIY'\n",
    "SAMPLE_RANGE_NAME = 'crm!A:Z'\n",
    "#aqui não precisa alterar nada\n",
    "class Athena: \n",
    "    \n",
    "    def __init__(self, session: boto3.Session):\n",
    "        \n",
    "        self.session = session\n",
    "    def read(self, database: str, file_path: str = None, query: str = None) -> pd.DataFrame:\n",
    "       \n",
    "        \n",
    "        log.info(f'-----------< read query >-----------')\n",
    "        log.info(f'Database: {database}')\n",
    "        try:\n",
    "            _query = open(file_path).read() if file_path else query\n",
    "            df = wr.athena.read_sql_query(\n",
    "                _query, \n",
    "                database=database,\n",
    "                workgroup = 'sales-ops',\n",
    "                boto3_session=self.session\n",
    "            )\n",
    "            log.info(f'DataFrame: {df.shape}')\n",
    "            log.info(f'-------------< done >--------------')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            log.error(f\"Something went wrong executing query Exception: {e}\")\n",
    "#aqui não precisa alterar nada\n",
    "session = boto3.Session(region_name='us-east-1')\n",
    "#aqui não precisa alterar nada\n",
    "athena = Athena(session)\n",
    "\n",
    "def sheets_to_csv(spreadsheet_id, range_name, csv_file_name):\n",
    "    \"\"\"Extrai valores de uma planilha Google Sheets e salva como CSV.\"\"\"\n",
    "    \n",
    "    creds = None\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            if not os.path.exists('client_secret.json'):\n",
    "                print(\"Arquivo 'client_secret.json' não encontrado.\")\n",
    "                return\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('client_secret.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    try:\n",
    "        service = build('sheets', 'v4', credentials=creds)\n",
    "        sheet = service.spreadsheets()\n",
    "        result = sheet.values().get(spreadsheetId=spreadsheet_id, range=range_name).execute()\n",
    "        values = result.get('values', [])\n",
    "\n",
    "        if not values:\n",
    "            print(f\"Nenhum dado encontrado para a planilha '{spreadsheet_id}' com intervalo '{range_name}'.\")\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(values[1:], columns=values[0])\n",
    "        df.to_csv(csv_file_name, index=False)\n",
    "        print(f\"Dados salvos como '{csv_file_name}'\")\n",
    "\n",
    "    except HttpError as error:\n",
    "        print(f\"Ocorreu um erro na API do Google Sheets: {error}\")\n",
    "\n",
    "def expand_extra_info_sheets(csv_file_name, mes):\n",
    "    # Ler o arquivo CSV\n",
    "    df = pd.read_csv(csv_file_name)\n",
    "    \n",
    "    \n",
    "   # Converter a coluna 'date_contact' para datetime\n",
    "    df['date_contact'] = pd.to_datetime(df['date_contact'].str.replace('Z', ''), format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce').dt.tz_localize('UTC')\n",
    "\n",
    "    df = df[df['date_contact'].dt.month == mes]\n",
    "    \n",
    "    if 'strategy_tag' in df.columns:\n",
    "    # Manter linhas onde a coluna 'strategy_tag' contém a string 'stockout'\n",
    "        df = df[df['strategy_tag'].str.contains('campaign', case=False, na=False)]\n",
    "\n",
    "\n",
    "    # Processar cada linha na coluna 'extra_info'\n",
    "    for index, row in df.iterrows():\n",
    "        # Verificar se a célula está vazia ou não é um JSON\n",
    "        if pd.isna(row['extra_info']) or not isinstance(row['extra_info'], str):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Tentar extrair o JSON\n",
    "            extra_info = json.loads(row['extra_info'])\n",
    "\n",
    "            # Tratar o JSON baseado na presença da chave 'extra_info'\n",
    "            if 'extra_info' in extra_info and isinstance(extra_info['extra_info'], str):\n",
    "                # Caso a chave 'extra_info' exista e seja uma string, decodificar o JSON aninhado\n",
    "                extra_info_dict = json.loads(extra_info['extra_info'])\n",
    "            else:\n",
    "                # Caso contrário, usar o JSON extraído diretamente\n",
    "                extra_info_dict = extra_info\n",
    "\n",
    "            # Adicionar cada chave do dicionário como uma nova coluna no date_contactFrame\n",
    "            for key, value in extra_info_dict.items():\n",
    "                df.loc[index, key] = value\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Erro ao decodificar JSON na linha {index}\")\n",
    "            continue\n",
    "\n",
    "    # Remover a coluna 'extra_info' original\n",
    "    df.drop(columns=['extra_info'], inplace=True)\n",
    "\n",
    "    # Salvar o date_contactFrame modificado no mesmo arquivo CSV\n",
    "    return df\n",
    "\n",
    "def sheets_to_dataframe(spreadsheet_id, range_name):\n",
    "    \"\"\"Extrai valores de uma planilha Google Sheets e salva como CSV.\"\"\"\n",
    "    \n",
    "    creds = None\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            if not os.path.exists('client_secret.json'):\n",
    "                print(\"Arquivo 'client_secret.json' não encontrado.\")\n",
    "                return\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('client_secret.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    try:\n",
    "        service = build('sheets', 'v4', credentials=creds)\n",
    "        sheet = service.spreadsheets()\n",
    "        result = sheet.values().get(spreadsheetId=spreadsheet_id, range=range_name).execute()\n",
    "        values = result.get('values', [])\n",
    "\n",
    "        if not values:\n",
    "            print(f\"Nenhum dado encontrado para a planilha '{spreadsheet_id}' com intervalo '{range_name}'.\")\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(values[1:], columns=values[0])\n",
    "        return df\n",
    "\n",
    "    except HttpError as error:\n",
    "        print(f\"Ocorreu um erro na API do Google Sheets: {error}\")\n",
    "\n",
    "def expand_extra_info(df):\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    if 'strategy_tag' in df.columns:\n",
    "    # Manter linhas onde a coluna 'strategy_tag' contém a string 'stockout'\n",
    "        df = df[df['strategy_tag'].str.contains('campaign', case=False, na=False)]\n",
    "\n",
    "\n",
    "    # Processar cada linha na coluna 'extra_info'\n",
    "    for index, row in df.iterrows():\n",
    "        # Verificar se a célula está vazia ou não é um JSON\n",
    "        if pd.isna(row['extra_info']) or not isinstance(row['extra_info'], str):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Tentar extrair o JSON\n",
    "            extra_info = json.loads(row['extra_info'])\n",
    "            # Verificar se o 'extra_info' é um dicionário ou uma string\n",
    "            if isinstance(extra_info, str):\n",
    "                # Se for uma string, tentar desserializar novamente\n",
    "                extra_info = json.loads(extra_info)\n",
    "\n",
    "\n",
    "            # Tratar o JSON baseado na presença da chave 'extra_info'\n",
    "            if 'extra_info' in extra_info and isinstance(extra_info['extra_info'], str):\n",
    "                # Caso a chave 'extra_info' exista e seja uma string, decodificar o JSON aninhado\n",
    "                extra_info_dict = json.loads(extra_info['extra_info'])\n",
    "            else:\n",
    "                # Caso contrário, usar o JSON extraído diretamente\n",
    "                extra_info_dict = extra_info\n",
    "\n",
    "            # Adicionar cada chave do dicionário como uma nova coluna no date_contactFrame\n",
    "            for key, value in extra_info_dict.items():\n",
    "                df.loc[index, key] = value\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Erro ao decodificar JSON na linha {index}\")\n",
    "            continue\n",
    "\n",
    "    # Remover a coluna 'extra_info' original\n",
    "    df.drop(columns=['extra_info'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def agrupar_skus_por_date_contact(df):\n",
    "    grouped_skus_by_date = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        date = row['date_contact'] \n",
    "        sku = row['sku']    \n",
    "\n",
    "        if date not in grouped_skus_by_date:\n",
    "            grouped_skus_by_date[date] = []\n",
    "\n",
    "        grouped_skus_by_date[date].append(sku)\n",
    "\n",
    "    return grouped_skus_by_date\n",
    "\n",
    "def generate_query_optin(df_filtrado):\n",
    "    grouped_date_contact = agrupar_skus_por_date_contact(df_filtrado)\n",
    "    \n",
    "    # Inicializar uma lista vazia para armazenar todos os date_contactframes\n",
    "    all_results = []\n",
    "    print('Executando query: optins pós disparo') \n",
    "    for date, skus in grouped_date_contact.items():\n",
    "        \n",
    "\n",
    "    # Divide a lista de sellers em pedaços de tamanho 2000\n",
    "        skus_chunks = [skus[i:i + 2000] for i in range(0, len(skus), 2000)]\n",
    "    \n",
    "        for skus_chunk in skus_chunks:\n",
    "            formatted_skus = ', '.join([f\"'{sku}'\" for sku in skus_chunk])\n",
    "\n",
    "            # Montar a query\n",
    "            optins_pos= f\"\"\"  select \n",
    "            distinct\n",
    "            cp.sku,\n",
    "            cc.id as campaign_id,\n",
    "            cc.olister_responsible_email,\n",
    "            1 as optin_active,\n",
    "            date_format(min(cp.created_at), '%Y-%m-%d') as dt_optin,\n",
    "            ct.label AS campaign_label\n",
    "            from datalake_silver.campaigns_api_campaigns_campaign cc\n",
    "            left join datalake_silver.campaigns_api_campaigns_campaignproducthistory cp on cc.id = cp.campaign_id\n",
    "            left join datalake_silver.campaigns_api_campaigns_tag as ct on cc.tag_id = ct.id\n",
    "            where cp.status in ('active','accepted')\n",
    "            and cc.updated_at = (\n",
    "                select max(updated_at) \n",
    "                from datalake_silver.campaigns_api_campaigns_campaign as ccc \n",
    "                where ccc.id = cc.id)\n",
    "            and cp.sku in ({formatted_skus})  AND cp.created_at BETWEEN DATE('{date}') AND date_add('day', -1, date_add('month', 1, date_trunc('month', DATE('{date}'))))\n",
    "            group by cp.sku, cc.id, cc.olister_responsible_email,ct.label\n",
    "    \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "            # Executar a query e armazenar o resultado\n",
    "            result_df = athena.read('datalake', query=optins_pos)\n",
    "            all_results.append(result_df)\n",
    "\n",
    "        # Combinar todos os date_contactframes resultantes\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "    return combined_df\n",
    "\n",
    "def agrupar_skus_optin_por_date_contact(df):\n",
    "    grouped_skus_by_date = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        date = row['dt_optin']\n",
    "        sku = row['sku']\n",
    "        id = row['campaign_id']\n",
    "\n",
    "        if date not in grouped_skus_by_date:\n",
    "            grouped_skus_by_date[date] = []\n",
    "\n",
    "        # Adiciona um dicionário contendo o par SKU e ID à lista da data correspondente\n",
    "        grouped_skus_by_date[date].append({'sku': sku, 'campaign_id': id})\n",
    "\n",
    "    return grouped_skus_by_date\n",
    "\n",
    "def generate_query_GMV(df_optins):\n",
    "    grouped_date_contact = agrupar_skus_optin_por_date_contact(df_optins)\n",
    "    \n",
    "    all_results = []\n",
    "    print('Executando query: gmv optins disparo') \n",
    "\n",
    "    for date, items in grouped_date_contact.items():\n",
    "        skus = [item['sku'] for item in items]\n",
    "        ids = [item['campaign_id'] for item in items]\n",
    "\n",
    "        formatted_skus = ', '.join([f\"'{sku}'\" for sku in skus])\n",
    "        formatted_ids = ', '.join([f\"'{id}'\" for id in ids])\n",
    "        \n",
    "        query_gmv  = f\"\"\"\n",
    "        select \n",
    "        soi.product_sku,\n",
    "        soi.campaign_id as campaign_id,\n",
    "        soi.id as order_id,\n",
    "        sum(cast(soi.freight_value as double) + cast(soi.price as double)) as gmv,\n",
    "        so.purchase_timestamp\n",
    "        from datalake_silver.orders_api_seller_orders_sellerorder as so\n",
    "        left join datalake_silver.orders_api_seller_orders_sellerorderitem as soi on soi.seller_order_id = so.id\n",
    "        left join datalake_silver.sellers_api_sellers_seller ss on ss.id = so.seller_id\n",
    "            \n",
    "        where 1=1\n",
    "        and so.status <> 'pending'\n",
    "        and so.cancelation_status = ''\n",
    "        and so.region = 'br'\n",
    "        and ss.plan_type <> '1P'\n",
    "        and so.purchase_timestamp >= date('{date}')\n",
    "        and soi.campaign_id in ({formatted_ids})  \n",
    "        and soi.product_sku in ({formatted_skus})\n",
    "        group by 1,2,3,5\n",
    "        \"\"\"\n",
    "        result_df = athena.read('datalake', query=query_gmv)\n",
    "        all_results.append(result_df)\n",
    "\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "def generate_query_GPA(df_pedidos_incrementais):\n",
    "    # Verificar se 'order_id' é a coluna correta. Se não for, substitua por sua coluna alvo.\n",
    "    order_ids = df_pedidos_incrementais['order_id'].unique()\n",
    "    all_results = []\n",
    "\n",
    "    # Dividir os IDs em chunks de tamanho 2000\n",
    "    ids_chunks = [order_ids[i:i + 2000] for i in range(0, len(order_ids), 2000)]\n",
    "\n",
    "    for ids_chunk in ids_chunks:\n",
    "        formatted_ids = ', '.join([f\"'{id}'\" for id in ids_chunk])\n",
    "\n",
    "        # Montar a query\n",
    "        gpa = f\"\"\"\n",
    "        WITH financial_control AS (    \n",
    "        SELECT\n",
    "            seller_order_item_code,\n",
    "            DATE_TRUNC('week', accounted_at) AS semana_competencia,\n",
    "            DATE_TRUNC('month', accounted_at) AS mes_competencia,\n",
    "            CAST(SUM(\n",
    "                CASE\n",
    "                    WHEN provision_type IN ('seller_transfer', 'seller_transfer_chargeback') THEN -1 * relative_amount\n",
    "                END) AS DECIMAL(24,2)) AS Gmv,\n",
    "            coalesce(CAST(SUM(\n",
    "                CASE\n",
    "                    WHEN provision_type IN ('seller_commission', 'seller_commission_chargeback', 'seller_commission_fine', 'seller_commission_fine_chargeback') THEN -1 * relative_amount\n",
    "                    ELSE 0\n",
    "                END) * (1 - 0.1125) AS DECIMAL(24,2)),0) AS Commission_net,\n",
    "            coalesce(CAST(SUM(\n",
    "                CASE\n",
    "                    WHEN provision_type IN ('seller_flat_fee', 'seller_flat_fee_chargeback', 'seller_flat_fee_fine', 'seller_flat_fee_fine_chargeback') THEN -1 * relative_amount\n",
    "                    ELSE 0\n",
    "                END) * (1 - 0.1125) AS DECIMAL(24,2)),0) AS Flat_fee_net,\n",
    "            coalesce(CAST(SUM(\n",
    "                CASE\n",
    "                    WHEN provision_type IN ('seller_markup', 'seller_markup_chargeback') THEN -1 * relative_amount\n",
    "                    ELSE 0\n",
    "                END) * (1 - 0.1125) AS DECIMAL(24,2)),0) AS Revenue_markup_net,\n",
    "            coalesce(CAST(SUM(\n",
    "                CASE\n",
    "                    WHEN provision_type IN ('seller_subscription', 'seller_subscription_chargeback') THEN -1 * relative_amount\n",
    "                    ELSE 0\n",
    "                END) * (1 - 0.0565) AS DECIMAL(24,2)),0) AS Subscription_net,\n",
    "            coalesce(CAST(SUM(\n",
    "                CASE\n",
    "                    WHEN provision_type IN ('marketplace_commission_discount', 'marketplace_commission', 'marketplace_commission_chargeback',\n",
    "                                            'marketplace_commission_fine', 'marketplace_commission_fine_chargeback',\n",
    "                                            'marketplace_flat_fee', 'marketplace_flat_fee_chargeback',\n",
    "                                            'marketplace_flat_fee_fine', 'marketplace_flat_fee_fine_chargeback') THEN relative_amount\n",
    "                    ELSE 0\n",
    "                END) * (1 - 0.0925) AS DECIMAL(24,2)) * -1,0) AS Net_COGS,\n",
    "            coalesce(CAST(SUM(\n",
    "                CASE\n",
    "                    WHEN provision_type IN ('seller_incentive_value', 'seller_incentive_value_chargeback', 'seller_subsidy', 'seller_subsidy_chargeback', 'seller_price_discount', 'seller_price_discount_chargeback', 'marketplace_subsidy', 'marketplace_subsidy_chargeback', 'seller_flat_freight_reduced', 'seller_flat_freight_reduced_chargeback', 'seller_freight_reduced', 'seller_freight_reduced_chargeback', 'seller_markup_incentive', 'seller_markup_incentive_chargeback', 'seller_operation_incentive', 'seller_operation_incentive_chargeback') THEN -1 * relative_amount\n",
    "                    ELSE 0\n",
    "                END) AS DECIMAL(24,2)),0) AS Sales_incentive_wihtout_ads,\n",
    "            coalesce(CAST(SUM(\n",
    "                CASE\n",
    "                    WHEN provision_type IN ('seller_flat_freight_deduction', 'seller_flat_freight_deduction_chargeback',\n",
    "                                            'seller_freight_buyer_deduction', 'seller_freight_buyer_deduction_chargeback',\n",
    "                                            'seller_freight_increased', 'seller_freight_increased_chargeback',\n",
    "                                            'carrier_quoted', 'carrier_quoted_chargeback', 'carrier_quoted_adjustment',\n",
    "                                            'driver_first_mile', 'driver_complements_first_mile') THEN -1 * relative_amount\n",
    "                    ELSE 0\n",
    "                END) AS DECIMAL(24,2)),0) AS freight_result\n",
    "        FROM datalake_silver.controller_api_accountingsellerstore_accountingsellerstore\n",
    "        GROUP BY seller_order_item_code, DATE_TRUNC('week', accounted_at), DATE_TRUNC('month', accounted_at)\n",
    "    )\n",
    "    SELECT\n",
    "        bio.product_id as sku,\n",
    "        bio.seller_order_item_id,\n",
    "        bio.seller_order_item_code,\n",
    "        ROUND(SUM(fc.Commission_net + fc.Flat_fee_net + fc.Revenue_markup_net + fc.Subscription_net + fc.Net_COGS + fc.Sales_incentive_wihtout_ads + fc.freight_result), 2) AS gross_profit_adjusted,\n",
    "        bio.campaign_id as campaign_id\n",
    "    FROM\n",
    "        datalake_gold.bio_orderitem bio\n",
    "    LEFT JOIN\n",
    "        financial_control fc ON bio.seller_order_item_code = fc.seller_order_item_code\n",
    "    WHERE bio.seller_order_item_id in ({formatted_ids})\n",
    "    GROUP BY\n",
    "        bio.seller_order_item_id, bio.seller_order_item_code, bio.campaign_id, bio.product_id;\n",
    "        \"\"\"\n",
    "        result_df = athena.read('datalake', query=gpa)\n",
    "        all_results.append(result_df)\n",
    "\n",
    "    # Concatenar todos os resultados parciais em um único DataFrame\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def consulta_crm(mes, strategy):\n",
    "    \n",
    "\n",
    "    # Montar a query\n",
    "    crm= f\"\"\"SELECT \n",
    "    n.id, \n",
    "    n.created_at as date_contact, \n",
    "    n.updated_at, \n",
    "    n.seller_id, \n",
    "    n.status, \n",
    "    n.phone_number, \n",
    "    n.sender_phone, \n",
    "    n.strategy as strategy_tag, \n",
    "    m.slug AS model_tag, \n",
    "    n.extra_info, \n",
    "    n.region\n",
    "FROM \n",
    "    datalake_silver.communicator_api_negotiations_negotiation n\n",
    "JOIN \n",
    "    datalake_silver.communicator_api_negotiations_negotiationmodel m \n",
    "ON \n",
    "    n.model_id = m.id\n",
    "WHERE \n",
    "    MONTH(n.created_at) = {mes} AND \n",
    "    n.strategy = '{strategy}';\"\"\"\n",
    "\n",
    "    # Executar a query e armazenar o resultado\n",
    "    df_crm = athena.read('datalake', query=crm)\n",
    "\n",
    "    return df_crm\n",
    "\n",
    "def run_campanhas_3kus_results(mes):\n",
    "    ### -----------------------> Extraindo Dados CRM lake\n",
    "    \n",
    "    if mes<= 4:\n",
    "            # The ID and range of the spreadsheet.\n",
    "            SAMPLE_SPREADSHEET_ID = '1TpT5aOE1cQkqA2GOKRUN2vwXXHt_GD5JRPjGCVLvvIY'\n",
    "            SAMPLE_RANGE_NAME = 'crm!A:Z'\n",
    "            sheets_to_csv(SAMPLE_SPREADSHEET_ID, SAMPLE_RANGE_NAME, 'histórico_extra_info.csv')\n",
    "            df= expand_extra_info_sheets('histórico_extra_info.csv', mes)\n",
    "    else:\n",
    "            df = consulta_crm(mes, 'campaign')\n",
    "\n",
    "            df = expand_extra_info(df)    \n",
    "            #df= expand_extra_info_sheets('histórico_extra_info.csv')\n",
    "\n",
    "    # Inicializar uma lista para armazenar as novas linhas\n",
    "    linhas_expandidas = []\n",
    "\n",
    "    # Iterar sobre cada linha do date_contactFrame original\n",
    "    for index, row in df.iterrows():\n",
    "        for sku in ['sku_1', 'sku_2', 'sku_3']:\n",
    "            if pd.notna(row[sku]):\n",
    "                # Adicionar uma nova linha na lista\n",
    "                linhas_expandidas.append({'seller_id': row['seller_id'], 'date_contact': row['date_contact'], 'sku': row[sku]})\n",
    "\n",
    "    # Converter a lista em um date_contactFrame\n",
    "    df = pd.DataFrame(linhas_expandidas)\n",
    "\n",
    "    # Filtrar o DataFrame para manter apenas as linhas onde 'sku' tem 16 ou mais caracteres\n",
    "    df_filtrado = df[df['sku'].apply(lambda x: len(str(x)) >= 16)].copy()\n",
    "\n",
    "\n",
    "    df_filtrado['date_contact'] = df_filtrado['date_contact'].astype(str).str[:10]\n",
    "\n",
    "    # Extrair apenas a parte da data e converter para datetime\n",
    "    df_filtrado['date_contact'] = pd.to_datetime(df_filtrado['date_contact'], errors='coerce')\n",
    "\n",
    "\n",
    "    # Ordenar o DataFrame por 'sku' e 'date_contact' em ordem decrescente para ter a data mais recente primeiro\n",
    "    df_filtrado = df_filtrado.sort_values(by=['sku', 'date_contact'], ascending=[False, True])\n",
    "\n",
    "    # Remover duplicatas, mantendo a primeira ocorrência (date_contact mais recente) para cada 'sku'\n",
    "    df_filtrado = df_filtrado.drop_duplicates(subset='sku', keep='first')\n",
    "\n",
    "    # Formatar a coluna 'date_contact' para mostrar apenas a data\n",
    "    df_filtrado['date_contact'] = df_filtrado['date_contact'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    print('Quantidade de produtos únicos na lista final: {}'.format(df_filtrado['sku'].nunique()))\n",
    "     \n",
    "    df_optins = generate_query_optin(df_filtrado)\n",
    "\n",
    "\n",
    "    df_gmv_campanha = generate_query_GMV(df_optins)\n",
    "    # # # #-----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    df_gpa_campanha = generate_query_GPA(df_gmv_campanha)\n",
    "\n",
    "\n",
    "    df_gmv_gpa = pd.merge(df_gmv_campanha,df_gpa_campanha,  left_on='order_id', right_on='seller_order_item_id', how='left')\n",
    "    df_gmv_gpa['campaign_key'] = df_gmv_gpa['product_sku'].astype(str) +  df_gmv_gpa['campaign_id_x'].astype(str)\n",
    "\n",
    "\n",
    "    df_merged = pd.merge(df_filtrado, df_optins, left_on='sku', right_on='sku', how='left')\n",
    "\n",
    "    df_merged['campaign_key'] = df_merged['sku'].astype(str) +  df_merged['campaign_id'].astype(str)\n",
    "\n",
    "\n",
    "    df_gmv_gpa = pd.merge(df_gmv_gpa, df_merged,  left_on='campaign_key', right_on='campaign_key', how='left')\n",
    "\n",
    "    colunas_para_apagar = ['sku_x', 'seller_order_item_id', 'seller_order_item_code', 'campaign_id_y', 'sku_y', 'campaign_id']\n",
    "\n",
    "# Apagando as colunas\n",
    "    df_gmv_gpa.drop(columns=colunas_para_apagar, inplace=True)\n",
    "\n",
    "    df_gmv_gpa = df_gmv_gpa.rename(columns={'campaign_id_x': 'campaign_id'})\n",
    "\n",
    "    df_gmv_gpa = df_gmv_gpa.drop_duplicates(subset='order_id', keep='first')\n",
    "    df_merged = pd.read_csv(f'df_campanha_top3_{mes}.csv')\n",
    "    df_merged.drop('olister_responsible_email', axis=1, inplace=True)\n",
    "    df_merged['updated_at'] = datetime.now().strftime('%Y-%m-%d')\n",
    "    df_merged.drop('campaign_key', axis=1, inplace=True)\n",
    "    print(f'Dataframe de campanhas pronto: df_campanha_top3_{mes}.csv')\n",
    "    df_merged.to_csv(f'df_campanha_top3_{mes}.csv', index=False)\n",
    "    \n",
    "    print(\"FIM\")\n",
    "\n",
    "def agrupa_meses_campanha():\n",
    "    # Listas para armazenar os DataFrames carregados\n",
    "    dfs_campanha = []\n",
    "    dfs_pedidos = []\n",
    "\n",
    "    for mes in range(3, 7):\n",
    "        # Processamento do DataFrame de campanha\n",
    "        try:\n",
    "            # Tentar carregar os DataFrames de campanha\n",
    "            df_campanha_top3 = pd.read_csv(f'df_campanha_top3_{mes}.csv')\n",
    "            dfs_campanha.append(df_campanha_top3)\n",
    "        except FileNotFoundError:\n",
    "            print(f'Arquivo df_campanha_top3_{mes}.csv não encontrado, pulando para o próximo mês.')\n",
    "\n",
    "        # Processamento do DataFrame de pedidos\n",
    "        try:\n",
    "            df_gmv_gpa = pd.read_csv(f'pedidos_campanhas_{mes}.csv')\n",
    "            dfs_pedidos.append(df_gmv_gpa)\n",
    "        except FileNotFoundError:\n",
    "            print(f'Arquivo pedidos_campanhas_{mes}.csv não encontrado, pulando para o próximo mês.')\n",
    "        \n",
    "        # Concatenar os DataFrames de campanhas da lista se houver algum DataFrame carregado\n",
    "        if dfs_campanha:\n",
    "            df_final_campanha = pd.concat(dfs_campanha, ignore_index=True)\n",
    "            # Salvar o DataFrame resultante em um novo arquivo CSV\n",
    "            df_final_campanha.to_csv('df_campanha_top3_all_year.csv', index=False)\n",
    "        else:\n",
    "            print(\"Nenhum arquivo de campanha encontrado para processar.\")\n",
    "        \n",
    "        # Concatenar os DataFrames de pedidos da lista se houver algum DataFrame carregado\n",
    "        if dfs_pedidos:\n",
    "            df_final_pedidos = pd.concat(dfs_pedidos, ignore_index=True)\n",
    "            # Salvar o DataFrame resultante em um novo arquivo CSV\n",
    "            df_final_pedidos.to_csv('df_campanha_pedidos_all_year.csv', index=False)\n",
    "        else:\n",
    "            print(\"Nenhum arquivo de pedidos encontrado para processar.\")\n",
    "    \n",
    "    print(\"FIM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Começa o script de resultados de Campanha\n",
      "Dados salvos como 'histórico_extra_info.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matheus.marchiore\\AppData\\Local\\Temp\\ipykernel_21300\\3086784642.py:103: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de produtos únicos na lista final: 67841\n",
      "Executando query: optins pós disparo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n",
      "ERROR:root:Something went wrong executing query Exception: An error occurred (ExpiredTokenException) when calling the DeleteTable operation: The security token included in the request is expired\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All objects passed were None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComeça o script de resultados de Campanha\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m13\u001b[39m):  \u001b[38;5;66;03m# De 1 a 12\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m       \u001b[43mrun_campanhas_3kus_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m agrupa_meses_campanha(mes)\n",
      "Cell \u001b[1;32mIn[7], line 514\u001b[0m, in \u001b[0;36mrun_campanhas_3kus_results\u001b[1;34m(mes)\u001b[0m\n\u001b[0;32m    510\u001b[0m df_filtrado[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_contact\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_filtrado[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_contact\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantidade de produtos únicos na lista final: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(df_filtrado[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msku\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()))\n\u001b[1;32m--> 514\u001b[0m df_optins \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_query_optin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_filtrado\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    517\u001b[0m df_gmv_campanha \u001b[38;5;241m=\u001b[39m generate_query_GMV(df_optins)\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m# # # #-----------------------------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 284\u001b[0m, in \u001b[0;36mgenerate_query_optin\u001b[1;34m(df_filtrado)\u001b[0m\n\u001b[0;32m    281\u001b[0m         all_results\u001b[38;5;241m.\u001b[39mappend(result_df)\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;66;03m# Combinar todos os date_contactframes resultantes\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m combined_df\n",
      "File \u001b[1;32mc:\\Users\\matheus.marchiore\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\matheus.marchiore\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32mc:\\Users\\matheus.marchiore\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:541\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    538\u001b[0m         keys \u001b[38;5;241m=\u001b[39m Index(clean_keys, name\u001b[38;5;241m=\u001b[39mname, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(keys, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 541\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll objects passed were None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m objs_list, keys\n",
      "\u001b[1;31mValueError\u001b[0m: All objects passed were None"
     ]
    }
   ],
   "source": [
    "print('Começa o script de resultados de Campanha')\n",
    "for mes in range(3, 13):  # De 1 a 12\n",
    "      run_campanhas_3kus_results(mes)\n",
    "agrupa_meses_campanha(mes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
